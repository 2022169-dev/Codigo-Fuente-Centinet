#!/usr/bin/env python3
import pandas as pd
import pyshark
import subprocess
import json
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime
import requests
import time
import psutil  # Para detección de interfaces

class NetworkAnalyzer:
    def __init__(self, es_url, wazuh_url, db_config):
        self.es_url = es_url
        self.wazuh_url = wazuh_url
        self.db_conn = None
        try:
            self.db_conn = psycopg2.connect(**db_config)
            print("Conectado a la base de datos PostgreSQL.")
        except psycopg2.Error as e:
            print(f"Error al conectar con PostgreSQL: {e}")

    def interface_detection(self):
        interfaces = psutil.net_if_addrs()
        for nombre, addrs in interfaces.items():
            for addr in addrs:
                if addr.family.name == 'AF_INET' and not addr.address.startswith('127.'):
                    return nombre
        return 'any'  # fallback

    def capture_packets(self, interface=None, count=100):
        """Capturar paquetes con pyshark"""
        if interface is None:
            interface = self.interface_detection()
        print(f"Usando interfaz: {interface}")

        capture = pyshark.LiveCapture(interface=interface)
        packets = []

        try:
            for packet in capture.sniff_continuously(packet_count=count):
                packet_info = {
                    'timestamp': str(packet.sniff_time),
                    'src_ip': packet.ip.src if hasattr(packet, 'ip') else None,
                    'dst_ip': packet.ip.dst if hasattr(packet, 'ip') else None,
                    'protocol': packet.highest_layer,
                    'length': int(packet.length) if hasattr(packet, 'length') else None,
                    'info_dns': packet.dns.qry_name if hasattr(packet, 'dns') and hasattr(packet.dns, 'qry_name') else None
                }
                packets.append(packet_info)
        except Exception as e:
            print(f"Error al capturar paquetes: {e}")

        return pd.DataFrame(packets)

    def nmap_scan(self, target):
        """Ejecutar escaneo Nmap"""
        try:
            cmd = f"nmap -sS -O {target} -oX -"
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            return result.stdout
        except Exception as e:
            print(f"Error en escaneo Nmap: {e}")
            return None

    def get_wazuh_token(self, user='wazuh', password='wazuh'):
        """Obtiene un token JWT desde Wazuh Manager"""
        url = f"{self.wazuh_url}/security/user/authenticate"
        try:
            response = requests.post(url, auth=(user, password), verify=False, timeout=5)
            if response.status_code == 200:
                token = response.json()['data']['token']
                return token
            else:
                print(f"Error al autenticar: {response.status_code} {response.text}")
        except requests.exceptions.RequestException as e:
            print(f"Error de conexión al obtener token: {e}")
        return None

    def send_to_wazuh(self, data):
        """Enviar datos a Wazuh usando token JWT"""
        token = self.get_wazuh_token()
        if not token:
            print("No se obtuvo token. Saltando envío a Wazuh.")
            return

        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        try:
            url = f"{self.wazuh_url}/agents"
            response = requests.post(url, json=data, headers=headers, verify=False, timeout=5)
            if response.status_code == 200:
                print("Datos enviados correctamente a Wazuh.")
            else:
                print(f"Error al enviar datos: {response.status_code} {response.text}")
        except requests.exceptions.RequestException as e:
            print(f"Error al enviar datos a Wazuh: {e}")

    def store_log(self, packet_info):
        """Guardar paquete en la tabla trafico_ip"""
        if not self.db_conn:
            print("No se pudo conectar a la base de datos. Saltando el log.")
            return

        cursor = self.db_conn.cursor()
        try:
            cursor.execute("""
                INSERT INTO trafico_ip (timestamp, origen_ip, destino_ip, protocolo, longitud, info_dns)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                packet_info.get('timestamp'),
                packet_info.get('src_ip'),
                packet_info.get('dst_ip'),
                packet_info.get('protocol'),
                packet_info.get('length'),
                packet_info.get('info_dns')
            ))
            self.db_conn.commit()
        except psycopg2.Error as e:
            self.db_conn.rollback()
            print(f"Error en la BD: {e}")
        finally:
            cursor.close()

if __name__ == '__main__':
    db_config = {
        "host": "localhost",
        "user": "analizador",
        "password": "exp0_centinet",
        "dbname": "logs_de_red"
    }

    es_url = "http://localhost:9200"
    wazuh_url = "http://localhost:55000/agents"

    analyzer = NetworkAnalyzer(es_url, wazuh_url, db_config)

    if analyzer.db_conn:
        print("Analizador de red en ejecución. Presiona Ctrl+C para detener.")

        try:
            while True:
                packets_df = analyzer.capture_packets(count=50)
                packets_json = packets_df.to_dict('records')

                if packets_json:
                    analyzer.send_to_wazuh(packets_json)
                    print(f"Enviados {len(packets_json)} paquetes a Wazuh.")

                    for pkt in packets_json:
                        analyzer.store_log(pkt)

                time.sleep(5)

        except KeyboardInterrupt:
            print("Analizador de red detenido.")
        finally:
            analyzer.db_conn.close()
    else:
        print("No se pudo iniciar el analizador debido a un error de conexión a la base de datos.")